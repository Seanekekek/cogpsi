{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras & Deep Learning\n",
    "- book : [Deep Learning with Python by Francois Chollet](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438)\n",
    "- source code : [github](https://github.com/fchollet/deep-learning-with-python-notebooks)\n",
    "- CogPsi #2 2019.3.26. Yoon Kyung Lee\n",
    "- [git repository and notebooks](https://github.com/yoonlee78/cogpsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./imgs/reuters.png\" width = 250 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue> 3.5 뉴스 기사 분류: 다중 분류 문제 </font>\n",
    "-----\n",
    "\n",
    "### 차례\n",
    "\n",
    "3.5.1. 로이터 데이터셋 <br>\n",
    "3.5.2. 데이터 준비 <br>\n",
    "3.5.3. 모델 구성 <br>\n",
    "3.5.4. 훈련 검증 <br>\n",
    "3.5.5. 새로운 데이터에 대해 예측하기 <br>\n",
    "3.5.6. 레이블과 손실을 다루는 다른 방법 <br>\n",
    "3.5.7. 충분히 중간층을 두어야 하는 이유 <br>\n",
    "3.5.8 ~9. 정리 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지난 시간\n",
    "\n",
    "완전 연결된 신경망을 사용하여 백터 입력을 2개의 클래스로 분류하는지 살펴보았다. 2개 이상의 클래스가 있을 때는 어떻게 할까?\n",
    "\n",
    "#### 이번 시간\n",
    "\n",
    "##### **<다중분류>** multiclass classification \n",
    "\n",
    "이 절에서는 로이터(Reuter)뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망을 만들어보겠다. \n",
    "\n",
    "각 데이터 포인트가 정확히 하나의 범주로 분류되기 때문에 **단일 레이블 다중 분류 (single-label, multiclass classification)** 문제로 본다.*\n",
    "\n",
    "----------\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "*참고: 만약, 각 데이터 포인트가 여러개의 범주(or 토픽) 에 속할 수 있다면 **다중 레이블 다중 분류 (multi-label, multiclass classification)**  문제가 된다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1. 로이터 데이터셋\n",
    "\n",
    "- 1986년에 로이터에서 공개한 짧은 뉴스기사와 토픽의 집합\n",
    "- Reuters-21578, https://bit.ly/2JPwSa0\n",
    "- 135개 토픽 중 샘플이 많은 것을 뽑아 간단하게 만들었으며 금융과 관련된 카테고리 \n",
    "- IMDB, MNIST와 마찬가지로 이 데이터셋은 케라스에 포함되어있음. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters,models,layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels),(test_data, test_labels) = reuters.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB데이터셋처럼 num_words = 10000 매개 변수는 데이터에서 가장 자주 등장하는 단어 1만개로 제한함.\n",
    "\n",
    "훈련 샘플과 테스트 샘플을 확인해보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB 리뷰처럼 각 샘플은 정수(int)로된 리스트이다. 각 단어에 대한 인덱스 값이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#궁금한 경우 각 인덱스가 무슨 단어를 뜻하는지 디코딩 가능하다. 어떻게 디코딩하는지 알아보자. \n",
    "#로이터 데이터셋을 텍스트로 디코딩하기 \n",
    "\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "\n",
    "#0,1,2는 '패딩','문서 시작','사전에 없음'을 위한 인덱스이므로 3을 뺀다. \n",
    "#제거되기 전의 모습을 보려면 join함수 안에 reverse_word_index.get(i)으로 대체. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_newswire #책에 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플에 연결된 레이블은 토픽의 인덱스로 0과 45 사이의 정수이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 백터로 변환한다. \n",
    "\n",
    "**참고** \n",
    "IMDB와 로이터 데이터셋은 미리 전체 데이터셋의 단어를 고유한 정수 인덱스로 바꾼 후에 ```train_data```와 ```test_data```로 나누어 놓은 것임. 일반적으로는 ```train_data```에서 구축한 어휘 사전으로  ```test_data```를 변환한다. 이렇게 하는 이유는 실전에서 샘플에 어떤 텍스트가 들어 있을지 알 수 없기 때문에 ```test dataset```의 어휘를 이용하면 낙관적으로 테스트를 평가하는 셈이 되기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 인코딩\n",
    "#import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# 벡터화된 훈련 데이터\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 벡터화된 테스트 데이터\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label -> vector \n",
    "\n",
    "- 레이블의 리스트를 정수 텐서로 변환하는 것\n",
    "- 원-핫 인코딩 (범주형 인코딩_categorical encoding), 자세한 건 6.1에서. \n",
    "    -- 이 경우 레이블의 원-핫 인코딩은 각 레이블의 인덱스 자리는 1이고 나머지는 모두 0인 벡터. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원-핫 인코딩\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# 훈련 레이블 벡터 변환\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "\n",
    "# 테스트 레이블 벡터 변환\n",
    "one_hot_test_labels = to_one_hot(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 Keras 내장 함수 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#범주 변수를 벡터화하는 keras 내장함수 \n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3. 모델 구성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 리뷰 분류(IMDB)문제와 비슷하게 짧은 텍스트를 분류하지만, 출력 클래스의 개수가 2개에서 46개로 늘어난 점이 다르다.즉, 출력 공간의 차원이 훨씬 커졌다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전처럼 Dense 층을 쌓으면 각 층은 이전 층의 출력에서 제공한 정보만 사용할 수 있다. 한 층이 분류 문제에 필요한 일부 정보를 누락하면 그 다음 층에서 이를 복원할 방법이 없다. 각 층은 잠재적으로 정보의 병목(information bottleneck)이 될 수 있다. \n",
    "\n",
    "이전 예제에서 16차원을 가진 중간층을 사용했지만 16차원 공간은 46개의 클래스를 분류하기에 너무 제약이 많을 수 있다. 이렇게 규모가 작은 층은 유용한 정보를 완전히 잃게 될 수 있는 '정보의 병목 지점'처럼 동작할 수 있다. \n",
    "\n",
    "그러므로 좀 더 규모가 큰 64개의 유닛(층)을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 층 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import models\n",
    "#from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서, \n",
    "``` python\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "```\n",
    "\n",
    "마지막 Dense층의 크기는 46인데, 각 입력 샘플에 대해 46차원의 벡터를 출력한다는 뜻. <br>\n",
    "이 벡터의 각 원소(차원)는 각기 다른 출력 클래스가 인코딩 된 것.\n",
    "\n",
    "또한, 마지막 층에 ```softmax ``` 활성화 함수가 사용됨. 각 입력 샘플마다 46개의 출력 클래스에 대한 확률 분포를 출력. 즉 46차원의 출력 벡터를 만들며 output[i]는 어떤 샘플이 클래스 i에 속할 확률. 46개의 값을 모두 더하면 1이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 손실 함수\n",
    "\n",
    "이런 문제에 적합한 손실 함수는 **categorical_crossentropy**이다. 이 함수는 두 확률 분포 사이의 거리를 측정한다. 여기에서는 네트워크가 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리이다. 두 분포 사이의 거리를 최소화하면 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 컴파일\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4. 훈련 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 검증 세트 \n",
    "\n",
    "훈련 데이터에서 1,000개의 샘플을 따로 떼어서 검증 세트로 사용하겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검증 세트 준비하기\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 모델 훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20번의 에포크로 모델 훈련\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 손실과 정확도 곡선 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련과 검증 손실 그리기 \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련과 검증 손실 그리기 \n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련과 검증 정확도 그리기\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 평가\n",
    "\n",
    "이 모델은 8번째 에포크 이후로 (즉, 9번째부터) 과대 적합 (overfitting)이 일어나는 것을 볼 수 있다. 9번의 에포크로 새로운 모델을 훈련하고 테스트 세트에서 평가해보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 모델 재훈련 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델을 처음부터 다시 훈련\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8, #에포크를 8로 조정\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 최종 결과 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대략 78%의 정확도를 달성함. \n",
    "균형 잡힌 이진 분류 문제에서 완전히 무작위로 분류하면 대략 50%의 정확도를 달성한다. 이 문제는 불균형한 데이터셋을 사용하므로 무작위로 분류하면 18% 정도 달성한다. 여기에 비하면 \n",
    "\n",
    "아래 예시에 비하면 꽤 좋은 편이라고 할 수 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy)))/ len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5. 새로운 데이터에 대해 예측하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 인스턴스의 predict 메서드는 46개의 토픽에 대한 확률 분포를 반환한다. \n",
    "\n",
    "테스트 데이터 전체에 대한 토픽을 예측해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#새로운 데이터에 대해 예측하기 \n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].shape #각 항목은 길이가 46인 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(predictions[0]) # 벡터의 원소의 합은 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 큰 값이 예측 클래스가 된다. 즉, 가장 확률이 높은 클래스이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.6. 레이블과 손실을 다루는 다른 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블을 인코딩하는 다른 방법을 앞서 소개했었다. 원 핫 인코딩 말고 다른 방법은 정수 텐서로 변환하는 것이다. \n",
    "\n",
    "train_labels와 test_labels는 정수(int)타입의 NumPy 배열이기 때문에 다시 np.array()함수를 사용할 필요는 없다. np.array() 함수는 np.asarray()함수와 동일하나 입력된 넘파이 배열의 복사본을 만들어 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방식을 사용하려면 손실 함수 하나만 바꾸면 된다. 위에서 사용한 ```categorical_crossentropy``` 손실 함수는 레이블이 범주형 인코딩되어 있는 것을 가정하기 때문에 정수 레이블을 사용할 때는 ```sparse_categorical_crossentropy```를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.7. 충분히 큰 중간층을 두어야 하는 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 언급한 바와 같이 마지막 출력이 46차원이기 때문에 중간층의 히든 유닛이 46개보다 많이 적어서는 안 된다. 46차원보다 훨씬 작은 중간층(예를 들어, 4차원)을 두면 정보의 병목이 어떻게 나타나는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 정확도의 최고 값은 <font color = red> **약 71%** </font>로 <font color = red> _8%정도 감소_</font>했다. <br> 이런 손실의 원인 대부분은 많은 정보 (클래스 46개의 분할 초평면을 복원하기에 충분한 정보)를 중간층의 저차원 표현 공간으로 압축하려고 했기 때문이다. 이 네트워크는 필요한 정보 대부분을 4차원 표현 안에 구겨 넣었지만 전부는 넣지 못한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.8. 선택: 추가 실험\n",
    "\n",
    "- 더 크거나 작은 층을 사용해보자 (32개, 128개 유닛..등)\n",
    "- 여기서는 2개의 은닉 층을 사용했다. 1개나 3개의 은닉 층을 사용해보세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.9. 정리 \n",
    "\n",
    "- 다음은 이 예제에서 배운 것들이다. \n",
    "\n",
    "    - N개의 클래스로 데이터 포인트를 분류하려면 네트워크의 마지막 Dense 층의 크기는 N이어야 한다. \n",
    "    - 단일 레이블, 다중 분류 문제에서는 N개의 클래스에 대한 확률 분포를 출력하기 위해 softmax 활성화 함수를 사용해야 한다. \n",
    "    -  이런 문제에서는 항상 범주형 크로스엔트로피 (categorical_crossentropy)를 사용해야한다. 이 함수는 모델이 출력한 확률 분포와 타깃 분포 사이이 거리를 최소화 해준다. \n",
    "    - 다중 분류에서 레이블을 다루는 두 가지 방법 : 1) 레이블을 범주형 인코딩(cateogorical_crossentropy)하거나, 정수로 인코딩(sparse_categorical_crossentropy)한다. \n",
    "    - 많은 수의 범주를 분류할 때 중간층의 크기가 너무 작아 네트워크에 정보의 병목이 생기지 않도록 주의한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
